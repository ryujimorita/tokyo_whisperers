@misc{huggingfaceReazonspeechDatasets,
	author = {Reazon Human Interaction Lab},
	title = {ReazonSpeech - Datasets at Hugging Face},
	url = {https://huggingface.co/datasets/reazon-research/reazonspeech},
	year = {},
}

@misc{reazon20240801ReazonSpeech,
	author = {Reazon Human Interaction Lab},
	title = {(2024-08-01) ReazonSpeech blog: Setting a New Standard in Japanese},
	url = {https://research.reazon.jp/blog/2024-08-01-ReazonSpeech.html},
	year = {},
}

@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{xu2023parameterefficientfinetuningmethodspretrained,
      title={Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: A Critical Review and Assessment}, 
      author={Lingling Xu and Haoran Xie and Si-Zhao Joe Qin and Xiaohui Tao and Fu Lee Wang},
      year={2023},
      eprint={2312.12148},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.12148}, 
}

@misc{githubWhispermodelcardmdMain,
	author = {{OpenAI}},
	title = {Whisper Model Card},
	year = {2024},
	url = {https://github.com/openai/whisper/blob/main/model-card.md},
	note = {[Accessed 09-12-2024]}
}

@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@misc{githubOpenaiWhisper,
	author = {{OpenAI}},
	title = {GitHub - OpenAI Whisper},
	year = {2024},
	url = {https://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages},
	note = {[Accessed 09-12-2024]}
}

@misc{githubReazonSpeech,
	author = {{Reazon Research}},
	title = {Massive Open Japanese Speech Corpus},
	year = {2024},
	url = {https://github.com/reazon-research/ReazonSpeech?tab=readme-ov-file#packages},
	note = {[Accessed 09-12-2024]}
}

@online{ardilaCommonVoiceMassivelyMultilingual2020,
  title = {Common {{Voice}}: {{A Massively-Multilingual Speech Corpus}}},
  shorttitle = {Common {{Voice}}},
  author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M. and Weber, Gregor},
  date = {2020-03-05},
  eprint = {1912.06670},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1912.06670},
  urldate = {2024-12-07},
  abstract = {The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla’s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 ± 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
}

@online{conneauFLEURSFewshotLearning2022,
  title = {{{FLEURS}}: {{Few-shot Learning Evaluation}} of {{Universal Representations}} of {{Speech}}},
  shorttitle = {{{FLEURS}}},
  author = {Conneau, Alexis and Ma, Min and Khanuja, Simran and Zhang, Yu and Axelrod, Vera and Dalmia, Siddharth and Riesa, Jason and Rivera, Clara and Bapna, Ankur},
  date = {2022-05-25},
  eprint = {2205.12446},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.12446},
  urldate = {2024-12-07},
  abstract = {We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark. FLEURS is an n-way parallel speech dataset in 102 languages built on top of the machine translation FLoRes-101 benchmark, with approximately 12 hours of speech supervision per language. FLEURS can be used for a variety of speech tasks, including Automatic Speech Recognition (ASR), Speech Language Identification (Speech LangID), Translation and Retrieval. In this paper, we provide baselines for the tasks based on multilingual pre-trained models like mSLAM. The goal of FLEURS is to enable speech technology in more languages and catalyze research in low-resource speech understanding.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
}
@online{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  date = {2021-10-16},
  eprint = {2106.09685},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2024-12-10},
  abstract = {An important paradigm of natural language processing consists of large-scale pretraining on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example – deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than finetuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
}

@online{holdingsReazonSpeechV21Setting2024,
  title = {ReazonSpeech v2.1: Setting a New Standard in Japanese ASR},
  author = {Holdings, Reazon},
  date = {2024-08-01},
  url = {https://research.reazon.jp/blog/2024-08-01-ReazonSpeech.html},
  urldate = {2024-12-07},
  langid = {japanese}
}

@article{radfordRobustSpeechRecognition,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  langid = {english},
}

@online{sonobeJSUTCorpusFree2017,
  title = {{{JSUT}} Corpus: Free Large-Scale {{Japanese}} Speech Corpus for End-to-End Speech Synthesis},
  shorttitle = {{{JSUT}} Corpus},
  author = {Sonobe, Ryosuke and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
  date = {2017-10-28},
  eprint = {1711.00354},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1711.00354},
  urldate = {2024-12-07},
  abstract = {Thanks to improvements in machine learning techniques including deep learning, a free large-scale speech corpus that can be shared between academic institutions and commercial companies has an important role. However, such a corpus for Japanese speech synthesis does not exist. In this paper, we designed a novel Japanese speech corpus, named the “JSUT corpus,” that is aimed at achieving end-to-end speech synthesis. The corpus consists of 10 hours of reading-style speech data and its transcription and covers all of the main pronunciations of daily-use Japanese characters. In this paper, we describe how we designed and analyzed the corpus. The corpus is freely available online.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
}

@article{yinReazonSpeechFreeMassive,
  title = {{{ReazonSpeech}}: {{A Free}} and {{Massive Corpus}} for {{Japanese ASR}}},
  author = {Yin, Yue and Mori, Daijiro and Fujimoto, Seiji and Holdings, Reazon},
  abstract = {ReazonSpeech is a 15,000-hour and continuously growing corpus collected from Japanese TV shows free for commercial usage. The automatic speech recognition (ASR) model trained on ReazonSpeech achieves state-of-the-art results with 8.23\% character error rate (CER) on JSUT basic5000[1] and 9.93\% on Common Voice[2] v8.0 test set, on par with the recently released Whisper[3] largev2 model. We released the dataset creation toolkit under Apache License 2.0 and made both the corpus and the pretrained ASR model freely available1）.},
  langid = {english},
}
@inproceedings{karitaLenientEvaluationJapanese2023,
  title = {Lenient {{Evaluation}} of {{Japanese Speech Recognition}}: {{Modeling Naturally Occurring Spelling Inconsistency}}},
  shorttitle = {Lenient {{Evaluation}} of {{Japanese Speech Recognition}}},
  booktitle = {Proceedings of the {{Workshop}} on {{Computation}} and {{Written Language}} ({{CAWL}} 2023)},
  author = {Karita, Shigeki and Sproat, Richard and Ishikawa, Haruko},
  date = {2023},
  pages = {61--70},
  publisher = {Association for Computational Linguistics},
  location = {Toronto, Canada},
  doi = {10.18653/v1/2023.cawl-1.8},
  url = {https://aclanthology.org/2023.cawl-1.8},
  urldate = {2024-12-08},
  abstract = {Word error rate (WER) and character error rate (CER) are standard metrics in Speech Recognition (ASR), but one problem has always been alternative spellings: If one’s system transcribes adviser whereas the ground truth has advisor, this will count as an error even though the two spellings really represent the same word.},
  eventtitle = {Proceedings of the {{Workshop}} on {{Computation}} and {{Written Language}} ({{CAWL}} 2023)},
  langid = {english},
}

@misc{IndroducingWhisper,
  author = {OpenAI},
  title = {Introducing Whisper},
  howpublished = {GitHub Repository},
  url = {https://openai.com/index/whisper/},
  year = {2022}
}

@misc{MediumWhisper,
  howpublished = {Medium},
  author = {David Cochard},
  title = {Whisper: Speech Recognition Model Capable of Recognizing 99 Languages},
  url = {https://medium.com/axinc-ai/whisper-speech-recognition-model-capable-of-recognizing-99-languages-5b5cf0197c16},
  year = {2023}
}

@misc{ReazonSpeechNemo,
  author = {Reazon Human Interaction Lab},
  title = {ReazonSpeechNemo Hugging Face Repository},
  howpublished = {Hugging Face Repository},
  url = {https://huggingface.co/reazon-research/reazonspeech-nemo-v2}
}

@misc{ReazonSpeechK2,
  author = {Reazon Human Interaction Lab},
  title = {ReazonSpeechK2 Hugging Face Repository},
  howpublished = {Hugging Face Repository},
  url = {https://huggingface.co/reazon-research/reazonspeech-k2-v2}
}

@misc{ReazonSpeechESPNet,
  author = {Reazon Human Interaction Lab},
  title = {ReazonSpeechESPNet Hugging Face Repository},
  howpublished = {Hugging Face Repository},
  url = {https://huggingface.co/reazon-research/reazonspeech-espnet-v2}
}

@misc{Icefall,
  author = {The Icefall Project Contributors},
  title = {Icefall Project Repository},
  howpublished = {GitHub Repository},
  url = {https://github.com/k2-fsa/icefall},
}

@article{SpecAugment,
  title={Specaugment: A simple data augmentation method for automatic speech recognition},
  author={Park, Daniel S and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D and Le, Quoc V},
  journal={arXiv preprint arXiv:1904.08779},
  year={2019}
}

